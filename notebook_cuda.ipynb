{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth -> pos using camera "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.options import AllConfigs \n",
    "from core.provider_objaverse import ObjaverseDataset as Dataset\n",
    "import tyro\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from core.options import AllConfigs\n",
    "# from core.models import LGM\n",
    "from core.models_fix_pretrained import LGM\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "import kiui\n",
    "\n",
    "from ipdb import set_trace as st\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import glob \n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_path=\"/mnt/kostas-graid/sw/envs/chenwang/workspace/lrm-zero123/assets/9000-9999\"\n",
    "scene_name_pattern = os.path.join(data_path, '*')\n",
    "scene_dirs = sorted(glob.glob(scene_name_pattern))\n",
    "for i, scene_path in enumerate(scene_dirs):\n",
    "    if i > 3:\n",
    "        break\n",
    "    scene_name = scene_path.split('/')[-2] if scene_path.endswith('/') else scene_path.split('/')[-1]\n",
    "    print(scene_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " # Filter out the --f argument if it's passed\n",
    "filtered_args = [\"big\"]\n",
    "# filtered_args += [arg for arg in sys.argv if not arg.startswith('--f=')]\n",
    "# print(filtered_args)\n",
    "\n",
    "opt = tyro.cli(AllConfigs, args=filtered_args)\n",
    "# print(opt)\n",
    "\n",
    " # training \n",
    "train_dataset = Dataset(opt, name=scene_path, training=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    # num_workers=opt.num_workers,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    if i > 1 :\n",
    "        break\n",
    "    print(data.keys())\n",
    "    print(data['cam_pos'].shape)\n",
    "    print(data['cam_view'].shape)\n",
    "\n",
    "    # use cam pos for projection\n",
    "    print(data['cam_view_proj'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_index = 2\n",
    "cam_view = data['cam_view'][:,cam_index] # choose one cam\n",
    "cam_poses = data['c2w'][:,cam_index] # choose one cam\n",
    "cam_view.shape, cam_poses.shape\n",
    "# cam_poses[:, :3, 1:3] *= -1\n",
    "cam_poses_neg = cam_poses.clone()\n",
    "cam_poses_neg[:, :3, 1:3] *= -1\n",
    "\n",
    "cam_view2 = torch.inverse(cam_poses).transpose(1, 2)\n",
    "cam_view3 = torch.inverse(cam_poses_neg).transpose(1, 2)\n",
    "torch.allclose(cam_view, cam_view2), torch.allclose(cam_view, cam_view3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether bmm is the same as @ \n",
    "N=20\n",
    "# c2w = cam_poses\n",
    "c2w = cam_poses_opengl\n",
    "pos = torch.rand(1,N,3)\n",
    "cam_pos = torch.cat([pos,\n",
    "           torch.ones((pos.shape[0], pos.shape[1], 1), device=c2w.device, dtype=torch.float32)\n",
    "            ], dim=2)\n",
    "pos_bmm = torch.bmm(cam_pos, c2w)\n",
    "pos_bmm_transpose = torch.bmm(cam_pos, c2w.transpose(1,2))\n",
    "pos_at = pos @ c2w[:, :3, :3].transpose(1,2) + c2w[:, :3, 3]\n",
    "pos_left = (c2w @ cam_pos.transpose(1,2)).transpose(1,2) # pos_at = pos_left = pos_bmm_transpose\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_left = pos_left[...,:3] / pos_left[...,3:]\n",
    "pos_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pos_bmm = pos_bmm[...,:3] / pos_bmm[...,3:]\n",
    "# pos_bmm_transpose = pos_bmm_transpose[...,:3] / pos_bmm_transpose[...,3:]\n",
    "pos_bmm, \n",
    "pos_bmm_transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(pos_at, pos_left), torch.allclose(pos_at, pos_bmm_transpose), torch.allclose(pos_bmm, pos_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the einops consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,V,H,W = 1, 6, 128, 128\n",
    "depth = torch.rand([B,V,1,H,W])\n",
    "depth_copy = depth.clone()\n",
    "print(depth.shape)\n",
    "depth = einops.rearrange(depth, 'b v c h w-> (b v) (h w) c', v=V, h=H, w=W)\n",
    "depth = einops.rearrange(depth, '(b v) (h w) c-> b v c h w', v=V, h=H, w=W)\n",
    "\n",
    "torch.allclose(depth_copy, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the depth_act used in the splatter image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = torch.rand(6,213,2)*10\n",
    "depth, torch.sigmoid(depth), torch.logit(torch.sigmoid(depth)), \\\n",
    "torch.allclose(depth,  torch.logit(torch.sigmoid(depth)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a vis of depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import kiui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ply\n",
    "# Load the tensor from the file\n",
    "# file_path = 'runs/LGM_optimize_splatter/debug2/00013-depth_offset-es10-Plat-patience_20-factor_0.5-eval_5-adamW-subset_0_-1_splat128-inV6-lossV20-lr0.006/9000-9999/0047fa5e44334d3e9bd171ad2bf95ff5/eval_pred_gs_600_0/raw_splatter_out_tensor.pt'\n",
    "# file_path = 'runs/LGM_optimize_splatter/debug2/00013-depth_offset-es10-Plat-patience_20-factor_0.5-eval_5-adamW-subset_0_-1_splat128-inV6-lossV20-lr0.006/9000-9999/0047fa5e44334d3e9bd171ad2bf95ff5/eval_pred_gs_1100_0_es/raw_splatter_out_tensor.pt'\n",
    "# file_path = 'runs/LGM_optimize_splatter/debug2/00013-depth_offset-es10-Plat-patience_20-factor_0.5-eval_5-adamW-subset_0_-1_splat128-inV6-lossV20-lr0.006/9000-9999/0047fa5e44334d3e9bd171ad2bf95ff5/eval_pred_gs_0_0/raw_splatter_out_tensor.pt'\n",
    "# file_path = 'runs/LGM_optimize_splatter/debug2/00013-depth_offset-es10-Plat-patience_20-factor_0.5-eval_5-adamW-subset_0_-1_splat128-inV6-lossV20-lr0.006/9000-9999/01529e7c8d7842c498f28a9d5927a0d6/eval_pred_gs_1410_0_es/raw_splatter_out_tensor.pt'\n",
    "file_path = 'runs/LGM_optimize_splatter/debug2/00013-depth_offset-es10-Plat-patience_20-factor_0.5-eval_5-adamW-subset_0_-1_splat128-inV6-lossV20-lr0.006/9000-9999/00e90648c8b94d0b9ea8a92858bd8e73/eval_pred_gs_1999_0/raw_splatter_out_tensor.pt'\n",
    "loaded_tensor = torch.load(file_path)\n",
    "\n",
    "print(loaded_tensor.shape)\n",
    "# get_depth and xyz offset from world pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# B, V, C, H, W = loaded_tensor.shape\n",
    "# for b in range(B):\n",
    "#     for v in range(V):       \n",
    "#         depth = loaded_tensor[b, v, 14:,... ]\n",
    "#         depth = depth.permute(1,2,0)\n",
    "#         print(depth.shape)\n",
    "#         kiui.write\n",
    "\n",
    "depth = loaded_tensor[:,:,14:] \n",
    "print(depth.min(), depth.max(), depth.mean(), depth.median())\n",
    "d_min, d_max = depth.min(), depth.max()\n",
    "# d_min, d_max = 0.5, 2\n",
    "depth = (depth - d_min) / (d_max - d_min)\n",
    "gt_images = depth.detach().cpu().numpy() # [B, V, 3, output_size, output_size]\n",
    "gt_images = gt_images.transpose(0, 3, 1, 4, 2).reshape(-1, gt_images.shape[1] * gt_images.shape[3], 1) # [B*output_size, V*output_size, 3]\n",
    "kiui.write_image('depth_epoch0.jpg', gt_images)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opacity = loaded_tensor[:,:, 3:4] \n",
    "print(opacity.min(), opacity.max(), opacity.mean(), opacity.median())\n",
    "# d_min, d_max = opacity.min(), opacity.max()\n",
    "\n",
    "d_min, d_max = -15, 5\n",
    "depth = (opacity - d_min) / (d_max - d_min)\n",
    "depth = 1 - depth.clamp(0,1)\n",
    "gt_images = depth.detach().cpu().numpy() # [B, V, 3, output_size, output_size]\n",
    "gt_images = gt_images.transpose(0, 3, 1, 4, 2).reshape(-1, gt_images.shape[1] * gt_images.shape[3], 1) # [B*output_size, V*output_size, 3]\n",
    "kiui.write_image('opacity_epoch_bucket.jpg', gt_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "depth_colormap = cm.jet(gt_images)  # This will produce an RGBA image\n",
    "print(gt_images.shape, depth_colormap.shape)\n",
    "# Remove the alpha channel\n",
    "depth_colormap = depth_colormap[..., 0, :3]\n",
    "plt.imsave('depth_colormap_birdman.png', depth_colormap)\n",
    "\n",
    "# If you need to display the colorized depth map instead of saving it\n",
    "plt.imshow(depth_colormap)\n",
    "plt.axis('off')  # Disable axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marigold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_availabl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tyro\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from core.options import AllConfigs\n",
    "from core.models_zero123plus import Zero123PlusGaussian, gt_attr_keys, start_indices, end_indices, fuse_splatters\n",
    "from core.models_zero123plus_code import Zero123PlusGaussianCode\n",
    "from core.models_zero123plus_code_unet_lora import Zero123PlusGaussianCodeUnetLora\n",
    "\n",
    "from core.models_fix_pretrained import LGM\n",
    "\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "from safetensors.torch import load_file\n",
    "from core.dataset_v4_batch import ObjaverseDataset as Dataset\n",
    "\n",
    "import kiui\n",
    "from datetime import datetime\n",
    "import torch.utils.tensorboard as tensorboard\n",
    "import shutil, os\n",
    "\n",
    "from ipdb import set_trace as st\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "from accelerate.utils import broadcast\n",
    "import re\n",
    "\n",
    "import numpy\n",
    "from PIL import Image\n",
    "from diffusers import DiffusionPipeline, EulerAncestralDiscreteScheduler\n",
    "# from zero123plus.img_to_mv_v3_my_decoder import to_rgb_image, unscale_image, unscale_latents\n",
    "\n",
    "import einops\n",
    "import rembg\n",
    "import requests\n",
    "\n",
    "from pytorch3d.transforms import quaternion_to_axis_angle, axis_angle_to_quaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_rgb_image(maybe_rgba: Image.Image):\n",
    "    if maybe_rgba.mode == 'RGB':\n",
    "        return maybe_rgba\n",
    "    elif maybe_rgba.mode == 'RGBA':\n",
    "        rgba = maybe_rgba\n",
    "        img = numpy.random.randint(127, 128, size=[rgba.size[1], rgba.size[0], 3], dtype=numpy.uint8)\n",
    "        img = Image.fromarray(img, 'RGB')\n",
    "        img.paste(rgba, mask=rgba.getchannel('A'))\n",
    "        return img\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported image type.\", maybe_rgba.mode)\n",
    "\n",
    "def unscale_latents(latents):\n",
    "    latents = latents / 0.75 + 0.22\n",
    "    return latents\n",
    "\n",
    "def unscale_image(image):\n",
    "    image = image / 0.5 * 0.8\n",
    "    return image\n",
    "\n",
    "def scale_image(image):\n",
    "    image = image * 0.5 / 0.8\n",
    "    return image\n",
    "\n",
    "def scale_latents(latents):\n",
    "    latents = (latents - 0.22) * 0.75\n",
    "    return latents\n",
    "\n",
    "def normalize_to_target(source_tensor, target_tensor):\n",
    "    # Calculate mean and standard deviation of source tensor\n",
    "    source_mean = torch.mean(source_tensor)\n",
    "    source_std = torch.std(source_tensor)\n",
    "\n",
    "    # Calculate mean and standard deviation of target tensor\n",
    "    target_mean = torch.mean(target_tensor)\n",
    "    target_std = torch.std(target_tensor)\n",
    "\n",
    "    # Normalize source tensor to target distribution\n",
    "    normalized_tensor = (source_tensor - source_mean) / source_std * target_std + target_mean\n",
    "\n",
    "    return normalized_tensor\n",
    "\n",
    "def fuse_splatters(splatters):\n",
    "    # fuse splatters\n",
    "    B, V, C, H, W = splatters.shape\n",
    "\n",
    "    x = splatters.permute(0, 1, 3, 4, 2).reshape(B, -1, 14)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_args = [\"big\"]\n",
    "opt = tyro.cli(AllConfigs,  args=filtered_args)\n",
    "\n",
    "from core.options import config_defaults\n",
    "big_config = config_defaults['big']\n",
    "\n",
    "# Now, big_config is an instance of Options with the 'big' configuration preset.\n",
    "# You can use it directly or modify it as needed\n",
    "big_config.num_epochs = 50  # Modify the number of epochs if needed\n",
    "big_config.batch_size = 1  # Change the batch size if needed\n",
    "\n",
    "print(opt)\n",
    "# Merge the 'big' config with the parsed command-line arguments\n",
    "for key, value in vars(opt).items():\n",
    "    if hasattr(big_config, key) and value is not None:\n",
    "        setattr(big_config, key, value)\n",
    "\n",
    "\n",
    "## modify the command line args\n",
    "DATA_DIR_BATCH_RENDERING='/mnt/kostas-graid/sw/envs/chenwang/workspace/lrm-zero123/assets/9000-9999'\n",
    "DATA_DIR_BATCH_SPLATTER_GT_ROOT='/home/xuyimeng/Repo/LGM/runs/splatter_gt'\n",
    "\n",
    "# Now modify opt based on the command line arguments\n",
    "opt.workspace = \"runs/zerp123plus_batch/workspace_inference2\"\n",
    "opt.lr = 2e-4\n",
    "opt.num_epochs = 10001\n",
    "opt.eval_iter = 5\n",
    "opt.save_iter = 5\n",
    "opt.lr_scheduler = \"Plat\"\n",
    "opt.lr_scheduler_patience = 100\n",
    "opt.lr_scheduler_factor = 0.7\n",
    "opt.prob_cam_jitter = 0\n",
    "opt.input_size = 320\n",
    "opt.num_input_views = 6\n",
    "opt.num_views = 20\n",
    "opt.lambda_splatter = 0\n",
    "opt.lambda_rendering = 1\n",
    "opt.lambda_alpha = 0\n",
    "opt.lambda_lpips = 0\n",
    "opt.desc = \"debug_encode_splatter\"\n",
    "opt.data_path_rendering = DATA_DIR_BATCH_RENDERING  # Replace with actual variable or constant\n",
    "opt.data_path_splatter_gt = DATA_DIR_BATCH_SPLATTER_GT_ROOT  # Replace with actual variable or constant\n",
    "opt.set_random_seed = True  # This implies there's some handling in the code to manage random seeds\n",
    "opt.batch_size = 1\n",
    "opt.num_workers = 1\n",
    "opt.plot_attribute_histgram = ('scale',)  # Assuming this accepts a tuple of strings\n",
    "opt.skip_predict_x0 = True\n",
    "opt.scale_act = \"biased_softplus\"\n",
    "opt.scale_act_bias = -3.\n",
    "opt.scale_bias_learnable = True\n",
    "opt.scale_clamp_max = -2\n",
    "opt.scale_clamp_min = -10\n",
    "opt.model_type = \"Zero123PlusGaussianCode\"\n",
    "opt.splatter_guidance_interval = 1\n",
    "opt.save_train_pred = -1\n",
    "opt.decode_splatter_to_128 = True\n",
    "opt.decoder_upblocks_interpolate_mode = \"last_layer\"\n",
    "opt.codes_from_diffusion = True\n",
    "opt.vae_on_splatter_image = True\n",
    "opt.overfit_one_scene = True\n",
    "\n",
    "print(opt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load diffusion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use model for model.gs.render()\n",
    "\n",
    "model = Zero123PlusGaussianCode(opt)\n",
    "from core.dataset_v4_code import ObjaverseDataset as Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fix this VAE encoder and decoder, find a good latent  \n",
    "\n",
    "from diffusers import DiffusionPipeline, EulerAncestralDiscreteScheduler\n",
    "# pipe = DiffusionPipeline.from_pretrained(\n",
    "#         \"sudo-ai/zero123plus-v1.1\", custom_pipeline=\"/mnt/kostas-graid/sw/envs/chenwang/workspace/diffgan/training/modules/zero123plus.py\",\n",
    "#         torch_dtype=torch.float32\n",
    "#     )\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"sudo-ai/zero123plus-v1.1\", custom_pipeline=\"sudo-ai/zero123plus-pipeline\",\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "pipe.to('cuda:0')\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(\n",
    "    pipe.scheduler.config, timestep_spacing='trailing'\n",
    ")\n",
    "output_path = f\"{opt.workspace}/zero123plus/outputs_v3_inference_my_decoder\"\n",
    "pipe.prepare()\n",
    "guidance_scale = 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debug rembg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{DATA_DIR_BATCH_RENDERING}/ffb0d644238b4c679658aa0ee46ac6da\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "poses = [np.load(f'{path}/{i:03d}.npy', allow_pickle=True).item() for i in range(1, 56)]\n",
    "elevations, azimuths = [-pose['elevation'] for pose in poses], [pose['azimuth'] for pose in poses]\n",
    "# imgs = [np.array(Image.open(f'{path}/{i:03d}.png')) / 255.0 for i in range(1, 7)]\n",
    "# imgs = [img[..., :3] * img[..., 3:4] + (1 - img[..., 3:4]) for img in imgs]\n",
    "\n",
    "# get imgs from pipeline\n",
    "cond = Image.open(f'{path}/{0:03d}.png')\n",
    "from IPython.display import display\n",
    "display(cond, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = pipe(cond, num_inference_steps=75).images[0]\n",
    "\n",
    "from torchvision import transforms\n",
    "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "# Normalization typically required for pre-trained models\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Combine ToTensor and Normalize transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # normalize\n",
    "])\n",
    "\n",
    "image = transform(result) # 3, 960, 640\n",
    "mv_image = einops.rearrange((image.clip(0,1)).cpu().numpy()*255, 'c (h2 h) (w2 w)-> (h2 w2) h w c', h2=3, w2=2).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_default_rays( device, elevation, azimuth):\n",
    "        \n",
    "    from kiui.cam import orbit_camera\n",
    "    from core.utils import get_rays\n",
    "\n",
    "    # cam_poses = np.stack([\n",
    "    #     orbit_camera(-30, 30, radius=self.opt.cam_radius),\n",
    "    #     orbit_camera(20, 90, radius=self.opt.cam_radius),\n",
    "    #     orbit_camera(-30, 150, radius=self.opt.cam_radius),\n",
    "    #     orbit_camera(20, 210, radius=self.opt.cam_radius),\n",
    "    #     orbit_camera(-30, 270, radius=self.opt.cam_radius),\n",
    "    #     orbit_camera(20, 330, radius=self.opt.cam_radius),\n",
    "    # ], axis=0) # [4, 4, 4]\n",
    "    cams = [orbit_camera(ele, azi, radius=opt.cam_radius) for (ele, azi) in zip(elevation, azimuth)]\n",
    "    cam_poses = np.stack(cams, axis=0)\n",
    "    cam_poses = torch.from_numpy(cam_poses)\n",
    "\n",
    "    rays_embeddings = []\n",
    "    for i in range(cam_poses.shape[0]):\n",
    "        rays_o, rays_d = get_rays(cam_poses[i], opt.input_size, opt.input_size, opt.fovy) # [h, w, 3]\n",
    "        rays_plucker = torch.cat([torch.cross(rays_o, rays_d, dim=-1), rays_d], dim=-1) # [h, w, 6]\n",
    "        rays_embeddings.append(rays_plucker)\n",
    "\n",
    "        ## visualize rays for plotting figure\n",
    "        # kiui.vis.plot_image(rays_d * 0.5 + 0.5, save=True)\n",
    "\n",
    "    rays_embeddings = torch.stack(rays_embeddings, dim=0).permute(0, 3, 1, 2).contiguous().to(device) # [V, 6, h, w]\n",
    "    \n",
    "    return rays_embeddings\n",
    "\n",
    "\n",
    "input_image = torch.from_numpy(mv_image).permute(0, 3, 1, 2).float().to(pipe.device) # [4, 3, 256, 256]\n",
    "input_image = F.interpolate(input_image, size=(opt.input_size, opt.input_size), mode='bilinear', align_corners=False)\n",
    "# if opt.model_type == 'LGM':\n",
    "#     rays_embeddings = model.prepare_default_rays(device, elevations[:6], azimuths[:6])\n",
    "#     input_image = TF.normalize(input_image, IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
    "#     input_image = torch.cat([input_image, rays_embeddings], dim=1).unsqueeze(0) # [1, 4, 9, H, W]\n",
    "rays_embeddings = prepare_default_rays(pipe.device, elevations[:6], azimuths[:6])\n",
    "input_image = torch.cat([input_image, rays_embeddings], dim=1).unsqueeze(0) # [1, 4, 9, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "mode_result = mode(mv_image[0].reshape(-1,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_result.mode + 1\n",
    "\n",
    "# # remok.sve bg\n",
    "# bg_mask_1 = np.all(mode_result.mode == mv_image[0], axis=-1) \n",
    "# bg_mask_2 = np.all((mode_result.mode + 1) == mv_image[0], axis=-1)\n",
    "# # bg_mask = np.any(bg_mask_1 or bg_mask_2)\n",
    "# bg_mask = (bg_mask_1 + bg_mask_2).clip(0,1)\n",
    "# # bg_mask.shape, bg_mask.astype(np.uint8)\n",
    "\n",
    "half_img = (mv_image[0].astype(np.float32) / 2. ).round().astype(np.uint8)\n",
    "bg_mask = np.all(64 == half_img, axis=-1) \n",
    "# bg_mask_2 = np.all((mode_result.mode + 1) == mv_image[0], axis=-1)\n",
    "plt.imshow(bg_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(mv_image[0])\n",
    "plt.imshow(bg_mask.astype(np.uint8)*255)\n",
    "\n",
    "img_no_bg = mv_image[0] * (1 - bg_mask[...,None]) + bg_mask[...,None]\n",
    "plt.imshow(img_no_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_image_no_bg = rembg.remove(einops.rearrange(mv_image, 'b h w c -> h (b w) c')) # .astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(img_no_bg)\n",
    "# np.array([3.6]).round().astype(np.uint8)\n",
    "plt.imshow(mv_image_no_bg[...,:-1])\n",
    "# plt.imshow(einops.rearrange(mv_image, 'b h w c -> h (b w) c'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_image_no_bg.shape\n",
    "mask = mv_image_no_bg[..., -1] > 0\n",
    "\n",
    "# from kiui.op import recenter\n",
    "# # recenter\n",
    "# image = recenter(mv_image_no_bg, mask, border_ratio=0.2)\n",
    "\n",
    "# generate mv\n",
    "# image = image.astype(np.float32) / 255.0\n",
    "image = mv_image_no_bg.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(opt, training=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=opt.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_dataloader):\n",
    "   print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_images = einops.rearrange(data[\"input\"], \"b (m n) c h w -> (b m h) (n w) c\", m=3, n=2).detach().cpu().numpy() # [B, V, 3, output_size, output_size]\n",
    "# gt_images = gt_images.transpose(0, 3, 1, 4, 2).reshape(-1, gt_images.shape[1] * gt_images.shape[3], gt_images.shape[2]) # [B*output_size, V*output_size, 3]\n",
    "kiui.write_image('data_input.jpg', gt_images)\n",
    "\n",
    "data[\"input\"].shape, gt_images.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_file = \"/home/xuyimeng/Repo/LGM/0_red_white_blue_gt_depth_fp32.npy\"\n",
    "depth_array_fp32 = np.load(depth_file)\n",
    "depth_array_fp32.shape, depth_array_fp32.min(), depth_array_fp32.max() # (960, 640), 0.0, 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splatters_mv = einops.rearrange(data[\"splatters_output\"], 'b (h2 w2) c h w -> b c (h2 h) (w2 w)', h2=3, w2=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bad init from LGM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # process the loaded splatters into 3-channel images\n",
    "gt_attr_keys = ['pos', 'opacity', 'scale', 'rotation', 'rgbs']\n",
    "start_indices = [0, 3, 4, 7, 11]\n",
    "end_indices = [3, 4, 7, 11, 14]\n",
    "attr_map = {key: (si, ei) for key, si, ei in zip (gt_attr_keys, start_indices, end_indices)}\n",
    "sp_min_max_dict = {\"z-depth\": (-0.7, 0.7), \n",
    "                   \"xy-offset\": (-0.7, 0.7), \n",
    "                   \"pos\": (-0.7, 0.7), \n",
    "                   \"scale\": (-10., -2.),\n",
    "                   }\n",
    "\n",
    "def prepare_3channel_images_to_encode(splatters_mv):\n",
    "   \n",
    "    splatter_3Channel_image = {}\n",
    "\n",
    "    for attr_to_encode, (start_i, end_i) in attr_map.items():\n",
    "    \n",
    "        splatter_attr = splatters_mv[:, start_i:end_i,...]\n",
    "\n",
    "        sp_min, sp_max = None, None\n",
    "\n",
    "        # process the channels\n",
    "        if end_i - start_i == 1:\n",
    "            print(f\"repeat attr {attr_to_encode} for 3 times\")\n",
    "            splatter_attr = splatter_attr.repeat(1, 3, 1, 1) # [0,1]\n",
    "        elif end_i - start_i == 3:\n",
    "            pass\n",
    "        elif attr_to_encode == \"xy-offset\":\n",
    "            # ## normalize to [0,1]\n",
    "            # sp_min, sp_max =  -1., 1.\n",
    "            # splatter_attr = (splatter_attr - sp_min) / (sp_max - sp_min)\n",
    "            ## cat one more dim\n",
    "            splatter_attr = torch.cat((splatter_attr, 0.5 * torch.ones_like(splatter_attr[:,0:1,...])), dim=1)\n",
    "        elif attr_to_encode == \"rotation\":\n",
    "            # st() # assert 4 is on the last dim\n",
    "            # quaternion to axis angle\n",
    "            quat = einops.rearrange(splatter_attr, 'b c h w -> b h w c')\n",
    "            axis_angle = quaternion_to_axis_angle(quat)\n",
    "            splatter_attr = einops.rearrange(axis_angle, 'b h w c -> b c h w')\n",
    "            # st()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"The dimension of {attr_to_encode} is problematic to encode\")\n",
    "\n",
    "        if \"scale\" in attr_to_encode:\n",
    "            # use log scale\n",
    "            splatter_attr = torch.log(splatter_attr)\n",
    "            \n",
    "            print(f\"{attr_to_encode} log min={splatter_attr.min()} max={splatter_attr.max()}\")\n",
    "            sp_min, sp_max =  -10., -2.\n",
    "            splatter_attr = (splatter_attr - sp_min) / (sp_max - sp_min) # [0,1]\n",
    "            splatter_attr = splatter_attr.clip(0,1)\n",
    "\n",
    "        elif attr_to_encode in [\"z-depth\", \"xy-offset\", \"pos\"] :\n",
    "            # sp_min, sp_max =  splatter_attr.min(), splatter_attr.max()\n",
    "            # sp_min, sp_max =  -1., 1.\n",
    "            sp_min, sp_max =  -0.7, 0.7\n",
    "            splatter_attr = (splatter_attr - sp_min) / (sp_max - sp_min)\n",
    "\n",
    "\n",
    "        print(f\"[{attr_to_encode}] in [0,1]: min={splatter_attr.min()} max={splatter_attr.max()}\")\n",
    "\n",
    "        sp_image = splatter_attr * 2 - 1 # [map to range [-1,1]]\n",
    "        print(f\"[{attr_to_encode}] in [-1, 1]: min={sp_image.min()} max={sp_image.max()}\")\n",
    "        \n",
    "        splatter_3Channel_image[attr_to_encode] = sp_image\n",
    "\n",
    "    return splatter_3Channel_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splatter_3Channel_image_returned = prepare_3channel_images_to_encode(splatters_mv)\n",
    "print()\n",
    "for attr_to_encode, splatter_attr in splatter_3Channel_image_returned.items():\n",
    "    print(attr_to_encode, splatter_attr.shape, splatter_attr.min(), splatter_attr.max())\n",
    "\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"pipe_vae\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the 3-channel splatter images into encoder for init latent code\n",
    "# Save image before encoding\n",
    "def encode_3channel_image_to_latents(sp_image):\n",
    "    \n",
    "    if not os.path.exists(os.path.join(output_path, name)):\n",
    "        os.makedirs(os.path.join(output_path, name), exist_ok=True)\n",
    "    mv_image = einops.rearrange((sp_image[0].clip(-1,1)+1).cpu().numpy()*127.5, 'c h w-> h w c').astype(np.uint8) \n",
    "    Image.fromarray(mv_image).save(os.path.join(output_path, f'{name}/{attr_to_encode}_to_encode.png'))\n",
    "\n",
    "    # encode: splatter attr -> latent \n",
    "    # sp_image_original = sp_image.clone()\n",
    "    sp_image = scale_image(sp_image.to(pipe.device))\n",
    "    sp_image = pipe.vae.encode(sp_image).latent_dist.sample() * pipe.vae.config.scaling_factor\n",
    "    latents = scale_latents(sp_image)\n",
    "\n",
    "    return latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get latents by encoding \n",
    "\n",
    "latents_all_attr_dict = {}\n",
    "for attr in gt_attr_keys:\n",
    "    latents_single_attr = encode_3channel_image_to_latents(splatter_3Channel_image_returned[attr])\n",
    "    latents_all_attr_dict.update({attr: latents_single_attr.detach().clone().requires_grad_(True)})\n",
    "\n",
    "latents_all_attr_tensor = torch.cat([latents_all_attr_dict[attr] for attr in gt_attr_keys])\n",
    "print(latents_all_attr_tensor.shape, len(gt_attr_keys)) # torch.Size([5, 4, 48, 32])\n",
    "assert latents_all_attr_tensor.shape[0] == len(gt_attr_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the latent code for optimization\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "latent_codes = latents_all_attr_tensor\n",
    "# optimizer = Adam([latent_codes], lr=0.01)\n",
    "optimizer = Adam([latents_all_attr_dict[attr] for attr in gt_attr_keys], lr=0.01)\n",
    "\n",
    "\n",
    "# Optimization loop\n",
    "num_iterations = 10\n",
    "acceptable_loss_threshold = 1\n",
    "\n",
    "decoded_attr_image_dict = {}\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    print(f\"[Iter {i}]\")\n",
    "    # Generate images from updated latent codes\n",
    "    # Step 2: Render the images from the latents\n",
    "    # Reshape latents if necessary and decode them\n",
    "    for attr, latents in latents_all_attr_dict.items():\n",
    "        if attr not in [\"scale\"]:\n",
    "            decoded_attr_image_dict.update({attr:decoded_images})\n",
    "            start_i, end_i = attr_map[attr]\n",
    "            splatter_attr = splatters_mv[:, start_i:end_i,...]\n",
    "            decoded_attr_image_dict.update({attr:splatter_attr.to(pipe.device)})\n",
    "            continue\n",
    "        if latents.requires_grad and latents.grad is not None:\n",
    "            print(f\"Gradient of {attr}: {latents.requires_grad} -- {latents.grad.norm().item()}\")\n",
    "\n",
    "        decoded_images = decode_single_latents(latents, attr_to_encode=attr)\n",
    "        decoded_attr_image_dict.update({attr:decoded_images})\n",
    "    \n",
    "   \n",
    "    # order the attributes back into splatter image\n",
    "    splatters_to_render = get_splatter_images_from_decoded_dict(decoded_attr_image_dict, group_scale=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Render the decoded images using the splatter model\n",
    "    gs_results = render_from_decoded_images(splatters_to_render, data=data)\n",
    "    \n",
    "    \n",
    "    # Step 3: Calculate the loss\n",
    "    # The target images should be the ground truth images you are trying to approximate\n",
    "    loss = calculate_loss(gs_results, data)\n",
    "    break\n",
    "    \n",
    "    # Step 4: Backward pass\n",
    "    loss.backward()  # Compute gradient of the loss w.r.t. latents\n",
    "    \n",
    "    # Step 5: Update the latents\n",
    "    optimizer.step()  # Perform a single optimization step\n",
    "    # check whether each latents has gradients\n",
    " \n",
    "    # Step 6: Print the loss or log it to observe convergence\n",
    "    print(f'Iteration {i}, Loss: {loss.item()}')\n",
    "\n",
    "    # Check for convergence or a stopping condition\n",
    "    if loss.item() < acceptable_loss_threshold:\n",
    "        print(\"Lower than acceptable_loss_threshold, success!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_single_latents(latents, attr_to_encode):\n",
    "    start_i, end_i = attr_map[attr_to_encode]\n",
    "\n",
    "    #  decode: latents -> platter attr\n",
    "    latents1 = unscale_latents(latents)\n",
    "    image = pipe.vae.decode(latents1 / pipe.vae.config.scaling_factor, return_dict=False)[0]\n",
    "    image = unscale_image(image)\n",
    "\n",
    "\n",
    "    # save decoded image\n",
    "    mv_image_numpy = einops.rearrange((image[0].clip(-1,1)+1).detach().cpu().numpy()*127.5, 'c h w-> h w c').astype(np.uint8) \n",
    "    Image.fromarray(mv_image_numpy).save(os.path.join(output_path, f'{name}/{attr_to_encode}_pred.png'))\n",
    "\n",
    "    # scale back to original range\n",
    "    mv_image = image # [b c h w], in [-1,1]\n",
    "\n",
    "    # if attr_to_encode in[ \"z-depth\", \"xy-offset\"]:\n",
    "    #     sp_image_o = mv_image\n",
    "    #     # st() # NOTE: try clip z-depth? No need, they are already within the range\n",
    "    # else:\n",
    "    sp_image_o = 0.5 * (mv_image + 1) # [map to range [0,1]]\n",
    "\n",
    "    print(f\"Decoded attr [-1,1] {attr_to_encode}: min={mv_image.min()} max={mv_image.max()}\")\n",
    "    print(f\"Decoded attr [0,1] {attr_to_encode}: min={sp_image_o.min()} max={sp_image_o.max()}\")\n",
    "\n",
    "    if \"scale\" in attr_to_encode:\n",
    "        # v2\n",
    "        sp_min, sp_max = sp_min_max_dict[\"scale\"]\n",
    "\n",
    "        sp_image_o = sp_image_o.clip(0,1) \n",
    "        sp_image_o = sp_image_o * (sp_max - sp_min) + sp_min\n",
    "        \n",
    "        print(f\"Decoded attr not clip [0,1] {attr_to_encode}: min={sp_image_o.min()} max={sp_image_o.max()}\")\n",
    "        sp_image_o = torch.exp(sp_image_o)\n",
    "        print(sp_min, sp_max)\n",
    "        print(f\"Decoded attr [unscaled] {attr_to_encode}: min={sp_image_o.min()} max={sp_image_o.max()}\")\n",
    "\n",
    "    # elif attr_to_encode == \"z-depth\":\n",
    "        # sp_image_o = sp_image_o * (sp_max - sp_min) + sp_min\n",
    "\n",
    "    elif attr_to_encode in[ \"z-depth\", \"xy-offset\", \"pos\"]:\n",
    "        sp_min, sp_max = sp_min_max_dict[attr_to_encode]\n",
    "        sp_image_o = sp_image_o * (sp_max - sp_min) + sp_min\n",
    "\n",
    "    if attr_to_encode == \"xy-offset\": \n",
    "        sp_image_o = sp_image_o[:,:2] # FIXME: ...,2??\n",
    "\n",
    "    if attr_to_encode == \"rotation\": \n",
    "        \n",
    "        ag = einops.rearrange(sp_image_o, 'b c h w -> b h w c')\n",
    "        quaternion = axis_angle_to_quaternion(ag)\n",
    "        sp_image_o = einops.rearrange(quaternion, 'b h w c -> b c h w')\n",
    "        # st()\n",
    "\n",
    "    if end_i - start_i == 1:\n",
    "        # print(torch.allclose(torch.mean(sp_image_o, dim=1, keepdim=True), sp_image_o))\n",
    "        # st()\n",
    "        print(f\"Decoded attr [unscaled, before mean] {attr_to_encode}: min={sp_image_o.min()} max={sp_image_o.max()}\")\n",
    "        sp_image_o = torch.mean(sp_image_o, dim=1, keepdim=True) # avg.\n",
    "\n",
    "\n",
    "        print(f\"Decoded attr [unscaled, after median] {attr_to_encode}: min={sp_image_o.min()} max={sp_image_o.max()}\")\n",
    "        \n",
    "  \n",
    "    print(f\"Decoded attr [unscaled] {attr_to_encode}: min={sp_image_o.min()} max={sp_image_o.max()}\")\n",
    "    \n",
    "    return sp_image_o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splatter_images_from_decoded_dict(decoded_attr_image_dict, group_scale=False):\n",
    "        \n",
    "    if not group_scale:\n",
    "        ordered_attr_list = [\"xy-offset\", \"z-depth\", # 0-3\n",
    "                            'opacity', # 3-4\n",
    "                            'scale-x', 'scale-y', 'scale-z', # 4-7\n",
    "                            \"rotation\", # 7-11\n",
    "                            \"rgbs\", # 11-14\n",
    "                            ] # must be an ordered list according to the channels\n",
    "    else:\n",
    "        ordered_attr_list = [\"pos\", # 0-3\n",
    "                            'opacity', # 3-4\n",
    "                            'scale', # 4-7\n",
    "                            \"rotation\", # 7-11\n",
    "                            \"rgbs\", # 11-14\n",
    "                            ] # must be an ordered list according to the channels\n",
    "    \n",
    "    attr_image_list = [decoded_attr_image_dict[attr] for attr in ordered_attr_list ]\n",
    "    # [print(t.shape) for t in attr_image_list]\n",
    "    splatter_mv = torch.cat(attr_image_list, dim=1)\n",
    "\n",
    "    ## reshape \n",
    "    splatters_to_render = einops.rearrange(splatter_mv, 'b c (h2 h) (w2 w) -> b (h2 w2) c h w', h2=3, w2=2) \n",
    "\n",
    "    return splatters_to_render\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_from_decoded_images(splatters_to_render, data):\n",
    "    gaussians = fuse_splatters(splatters_to_render)\n",
    "    bg_color = torch.ones(3, dtype=torch.float32, device=gaussians.device) * 0.5\n",
    "    gs_results = model.gs.render(gaussians, data['cam_view'], data['cam_view_proj'], data['cam_pos'], bg_color=bg_color)\n",
    "                    \n",
    "    return gs_results\n",
    "\n",
    "def calculate_loss(gs_results, data):\n",
    "    results = {}\n",
    "    loss = 0\n",
    "    \n",
    "    pred_images = gs_results['image'] # [B, V, C, output_size, output_size]\n",
    "    pred_alphas = gs_results['alpha'] # [B, V, 1, output_size, output_size]\n",
    "\n",
    "    gt_images = data['images_output'] # [B, V, 3, output_size, output_size], ground-truth novel views\n",
    "    gt_masks = data['masks_output'] \n",
    "\n",
    "    gt_images = gt_images * gt_masks + (1 - gt_masks) # NOTE: white bg\n",
    "\n",
    "    loss_mse_rendering = F.mse_loss(pred_images, gt_images) + F.mse_loss(pred_alphas, gt_masks)\n",
    "    results['loss_rendering'] = loss_mse_rendering\n",
    "\n",
    "    loss_lpips = model.lpips_loss(\n",
    "    # gt_images.view(-1, 3, self.opt.output_size, self.opt.output_size) * 2 - 1,\n",
    "    # pred_images.view(-1, 3, self.opt.output_size, self.opt.output_size) * 2 - 1,\n",
    "    # downsampled to at most 256 to reduce memory cost\n",
    "    \n",
    "    # FIXME: change the dim to 14 for splatter imaegs\n",
    "    F.interpolate(gt_images.view(-1, 3, opt.output_size, opt.output_size) * 2 - 1, (256, 256), mode='bilinear', align_corners=False), \n",
    "    F.interpolate(pred_images.view(-1, 3, opt.output_size, opt.output_size) * 2 - 1, (256, 256), mode='bilinear', align_corners=False),\n",
    "    ).mean()\n",
    "    results['loss_lpips'] = loss_lpips\n",
    "    loss = loss + opt.lambda_lpips * loss_lpips\n",
    "  \n",
    "    print(f\"loss lpips:{loss_lpips}\")\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear CUDA memory\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splatter_attr.device\n",
    "#  os.path.exists(os.path.join(output_path, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fake init from RGB + depth + constant others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rgb\n",
    "gt_images = einops.rearrange(data[\"input\"], \"b (m n) c h w -> (b m h) (n w) c\", m=3, n=2).detach().cpu().numpy() # [B, V, 3, output_size, output_size]\n",
    "# gt_images = gt_images.transpose(0, 3, 1, 4, 2).reshape(-1, gt_images.shape[1] * gt_images.shape[3], gt_images.shape[2]) # [B*output_size, V*output_size, 3]\n",
    "kiui.write_image('data_input.jpg', gt_images)\n",
    "\n",
    "data[\"input\"].shape, gt_images.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth \n",
    "## download marigold demo file\n",
    "depth_file = \"/home/xuyimeng/Repo/LGM/0_red_white_blue_gt_depth_fp32.npy\"\n",
    "depth_array_fp32 = np.load(depth_file)\n",
    "depth_array_fp32.shape, depth_array_fp32.min(), depth_array_fp32.max() # (960, 640), 0.0, 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets==7.7.1 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth \n",
    "## load meigold pipeline\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import shutil\n",
    "\n",
    "ckpt_dic = {\n",
    "    \"Original (higher quality)\": \"prs-eth/marigold-v1-0\",\n",
    "    \"LCM (faster)\": \"prs-eth/marigold-lcm-v1-0\",\n",
    "}\n",
    "\n",
    "ckpt_name = 'LCM (faster)'\n",
    "ckpt_path = ckpt_dic[ckpt_name]\n",
    "w = widgets.Dropdown(\n",
    "    options=['Original (higher quality)', 'LCM (faster)'],\n",
    "    value=ckpt_name,\n",
    "    description='Checkpoint:',\n",
    ")\n",
    "\n",
    "\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        ckpt_name = change['new']\n",
    "        ckpt_path = ckpt_dic[ckpt_name]\n",
    "        # print(f\"Change to checkpoint: {ckpt_name} ({ckpt_path})\")\n",
    "\n",
    "w.observe(on_change)\n",
    "\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    ckpt_path,\n",
    "    custom_pipeline=\"marigold_depth_estimation\"\n",
    ")\n",
    "\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
